{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Code (Run first)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Capture the output but do not do anything with it\n",
    "# Just to avoid cluttering the output\n",
    "\n",
    "%pip install python-terrier==0.9.2\n",
    "%pip install semanticscholar\n",
    "%pip install gensim\n",
    "%pip install seaborn\n",
    "%pip install KrovetzStemmer\n",
    "%pip install nltk\n",
    "%pip install langdetect\n",
    "%pip install Unidecode\n",
    "%pip install pandas\n",
    "%pip install pymed\n",
    "\n",
    "# Load all packages and initialize pyTerrier\n",
    "from langdetect import detect\n",
    "from unidecode import unidecode\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from krovetzstemmer import Stemmer\n",
    "from semanticscholar import SemanticScholar\n",
    "from gensim.models import Word2Vec\n",
    "from itertools import islice, chain\n",
    "from tqdm import tqdm\n",
    "from pyterrier.measures import *\n",
    "from pymed import PubMed\n",
    "import requests\n",
    "import os\n",
    "import pyterrier as pt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import time\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If not done already download the required data\n",
    "nltk.download()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vorgehen\n",
    "\n",
    "## Allgemein\n",
    "\n",
    "TREC-Covid-Queries verwenden und erweitern um weitere Anfrageterme, um die Retrieval-Performance zu verbessern (Query Expansion).\n",
    "\n",
    "Für die QE müssen Termkandidaten (\"set of C\" c_1, c_2, c_3, ...) bestimmt werden, die anschließend gerankt werden.\n",
    "D.h. ihr sendet zunächst die Standard-Query ab und später nochmal für die finale Evaluierung, die Query mit Termerweiterungen.\n",
    "\n",
    "## QE\n",
    "\n",
    "Wenn wenig Zeit:\n",
    "Nur die globale Variante evaluieren, wobei ja der \"Claim for Fame\" im Paper ist, dass die lokale Methode bessere Ergebnisse liefert.\n",
    "\n",
    "### LOKALE QE\n",
    "\n",
    "Word2Vec-Embeddings auf TREC-Covid trainieren (https://radimrehurek.com/gensim/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLOBALE QE\n",
    "\n",
    "Standardvariante wie z.B. auf Basis von Wikipedia.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just for testing: Google News word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('flu_virus', 0.779543936252594),\n",
       " ('viruses', 0.768750011920929),\n",
       " ('H#N#_virus', 0.7382203340530396)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the word2vec model\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim import models\n",
    "global_model = models.KeyedVectors.load_word2vec_format(\"external_models/GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "#global_model = Word2Vec.load(\"external_models/model.bin\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cough', 0.6782324910163879),\n",
       " ('coughs', 0.6672220826148987),\n",
       " ('sneezing', 0.6352056860923767)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.most_similar('coughing', topn=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global QE GLOVE Wiki Gigaword\n",
    "Wikipedia 2014 + Gigaword 5 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Standard-Query an den Index für erstes Ranking\n",
    "\n",
    "Aus dem Title des Topics, quasi wie in der Standard-Pipeline in Pyterrier.\n",
    "Paper Inverse Document Frequency model (InL2)\n",
    "Zuerst BM25.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Erstellung der Termkandidaten\n",
    "\n",
    "## 2.1 Top-3 Dokumente\n",
    "\n",
    "Alle Terme (ausschließlich der Stoppworte).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Terme der Referenzen\n",
    "\n",
    "Alle Terme aus den Referenzen der 3 Dokumente (Terme der Titel).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Erweiterung mit Co-Autoren\n",
    "\n",
    "Über die Koautoren können noch weitere Dokumente bzw. die entsprechenden Terme hinzugefügt werden. Aus Zeitgründen könnte man vielleicht auch diesen Schritt weglassen, wenn es zu viel Aufwand ist über Koautoren weitere Dokumente zu finden. Bei der Implementierung scheint ihr ja aber schon recht weit zu sein. Die \"relevanten Paper der Autoren\" sind einfach die Top-k Dokumente oder möglicherweise alle zusätzlichen Papers, die über die Koautorenschaft gefunden werden, gemeint. Daher ergibt sich auf der Name PSEUDO-Relevanz-Feedback, da einfach angenommen wird, dass die Top-Treffer alle relevant sind.\n",
    "\n",
    "Referenzen = Quellen (Alle im Paper direkt zitierten Quellen)\n",
    "\n",
    "Paper der Co-Autoren = Alle Autoren des Papers und deren Paper\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Ranking aller Terme\n",
    "\n",
    "\n",
    "\n",
    "Paper: Bo1\n",
    "Top-k Terme auswählen (k selbst wählen).\n",
    "Ergebnis: Potentielle Kandidaten für eine Termwerweiterung\n",
    "\n",
    "Bezug zur Mail:\n",
    "\n",
    "Bei 2.1.4\n",
    "    Wir nehmen alle Terme (topK abstracts, topK references titles, topK authors relevant papers abstracts) und ranken mit bo1.\n",
    "    Nehmen topK (3)\n",
    "\n",
    "Bei 2.2.1\n",
    "    Wir nehmen alle Terme (s.o.)\n",
    "    Trainieren Word2Vec\n",
    "    bo1 ranken\n",
    "    Nehmen topK (3)\n",
    "\n",
    "=> Kombinieren und topK nehmen und als Queries absenden.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5 Word2Vec-Modell mit Terms nachtrainieren\n",
    "\n",
    "Über die Kosinusähnlichkeit der Embeddings der ursprünglichen Anfrageterme bestimmt ihr nun weitere Terme aus dem Word2Vec-Modell. Diese Termkandidaten werden dann wie in 2.1.4 über Bo1 o.ä. gerankt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking\n",
    "bm25 = pt.BatchRetrieve(trec_covid_index, wmodel='BM25')\n",
    "bo1 = pt.rewrite.Bo1QueryExpansion(trec_covid_index)\n",
    "pipelineQE = bm25 >> bo1 >> bm25\n",
    "\n",
    "res = pipelineQE.transform(all_terms_expanded_df)\n",
    "all_terms_expanded_ranked_results = res.sort_values(by=\"score\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docid</th>\n",
       "      <th>docno</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>query_0</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1046469</th>\n",
       "      <td>2124</td>\n",
       "      <td>109806</td>\n",
       "      <td>k1sh5aqd</td>\n",
       "      <td>1</td>\n",
       "      <td>89.591816</td>\n",
       "      <td>han</td>\n",
       "      <td>applypipeline:off han^1.624118669 sufrido^0.96...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046470</th>\n",
       "      <td>2124</td>\n",
       "      <td>109807</td>\n",
       "      <td>0odga0w9</td>\n",
       "      <td>2</td>\n",
       "      <td>89.591816</td>\n",
       "      <td>han</td>\n",
       "      <td>applypipeline:off han^1.624118669 sufrido^0.96...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046468</th>\n",
       "      <td>2124</td>\n",
       "      <td>109805</td>\n",
       "      <td>ndco6jv2</td>\n",
       "      <td>0</td>\n",
       "      <td>89.591816</td>\n",
       "      <td>han</td>\n",
       "      <td>applypipeline:off han^1.624118669 sufrido^0.96...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4803964</th>\n",
       "      <td>6161</td>\n",
       "      <td>114562</td>\n",
       "      <td>vcsu27al</td>\n",
       "      <td>0</td>\n",
       "      <td>82.417343</td>\n",
       "      <td>form</td>\n",
       "      <td>applypipeline:off form^1.737729974 watersh^0.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3209808</th>\n",
       "      <td>4456</td>\n",
       "      <td>114562</td>\n",
       "      <td>vcsu27al</td>\n",
       "      <td>0</td>\n",
       "      <td>82.417343</td>\n",
       "      <td>form</td>\n",
       "      <td>applypipeline:off form^1.737729974 watersh^0.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4838390</th>\n",
       "      <td>6199</td>\n",
       "      <td>2272</td>\n",
       "      <td>e7xwb03g</td>\n",
       "      <td>426</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>send</td>\n",
       "      <td>applypipeline:off send^1.637534024 so^0.000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4838389</th>\n",
       "      <td>6199</td>\n",
       "      <td>2271</td>\n",
       "      <td>x4z0uj0g</td>\n",
       "      <td>425</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>send</td>\n",
       "      <td>applypipeline:off send^1.637534024 so^0.000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4838388</th>\n",
       "      <td>6199</td>\n",
       "      <td>2265</td>\n",
       "      <td>38rmx65j</td>\n",
       "      <td>424</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>send</td>\n",
       "      <td>applypipeline:off send^1.637534024 so^0.000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4838387</th>\n",
       "      <td>6199</td>\n",
       "      <td>2262</td>\n",
       "      <td>a4bgut8h</td>\n",
       "      <td>423</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>send</td>\n",
       "      <td>applypipeline:off send^1.637534024 so^0.000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2049661</th>\n",
       "      <td>324</td>\n",
       "      <td>80561</td>\n",
       "      <td>8spgv2qr</td>\n",
       "      <td>633</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>hiding</td>\n",
       "      <td>applypipeline:off hide^1.674674547 10^0.064851...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6345016 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          qid   docid     docno  rank      score query_0  \\\n",
       "1046469  2124  109806  k1sh5aqd     1  89.591816     han   \n",
       "1046470  2124  109807  0odga0w9     2  89.591816     han   \n",
       "1046468  2124  109805  ndco6jv2     0  89.591816     han   \n",
       "4803964  6161  114562  vcsu27al     0  82.417343    form   \n",
       "3209808  4456  114562  vcsu27al     0  82.417343    form   \n",
       "...       ...     ...       ...   ...        ...     ...   \n",
       "4838390  6199    2272  e7xwb03g   426   0.000000    send   \n",
       "4838389  6199    2271  x4z0uj0g   425   0.000000    send   \n",
       "4838388  6199    2265  38rmx65j   424   0.000000    send   \n",
       "4838387  6199    2262  a4bgut8h   423   0.000000    send   \n",
       "2049661   324   80561  8spgv2qr   633   0.000000  hiding   \n",
       "\n",
       "                                                     query  \n",
       "1046469  applypipeline:off han^1.624118669 sufrido^0.96...  \n",
       "1046470  applypipeline:off han^1.624118669 sufrido^0.96...  \n",
       "1046468  applypipeline:off han^1.624118669 sufrido^0.96...  \n",
       "4803964  applypipeline:off form^1.737729974 watersh^0.8...  \n",
       "3209808  applypipeline:off form^1.737729974 watersh^0.8...  \n",
       "...                                                    ...  \n",
       "4838390  applypipeline:off send^1.637534024 so^0.000000...  \n",
       "4838389  applypipeline:off send^1.637534024 so^0.000000...  \n",
       "4838388  applypipeline:off send^1.637534024 so^0.000000...  \n",
       "4838387  applypipeline:off send^1.637534024 so^0.000000...  \n",
       "2049661  applypipeline:off hide^1.674674547 10^0.064851...  \n",
       "\n",
       "[6345016 rows x 7 columns]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_terms_expanded_ranked_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.6 Zwei Sets zusammenfügen und Top-k für QE der ursprünglichen Query nutzen\n",
    "\n",
    "Diese jeweils für die 50 Topics absenden.\n",
    "\n",
    "Ergebnis: Die finalen Rankings, die dann ausgewertet werden können.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae50ba892a007039ee8b8cd0574fd987a1c048ef5b59b149546a5ae9fb6dc134"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
