{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Code (Run first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Capture the output but do not do anything with it\n",
    "# Just to avoid cluttering the output\n",
    "%pip install - -upgrade git+https: // github.com/terrierteam/pyterrier_doc2query.git\n",
    "%pip install python-terrier == 0.9.2\n",
    "%pip install semanticscholar\n",
    "%pip install gensim\n",
    "%pip install seaborn\n",
    "\n",
    "# Load all packages and initialize pyTerrier\n",
    "from gensim.models import Word2Vec\n",
    "from semanticscholar import SemanticScholar\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyterrier as pt\n",
    "import os\n",
    "\n",
    "if not pt.started():\n",
    "    pt.init()\n",
    "\n",
    "index_count = 0\n",
    "# Helper function to intialize multiple indices\n",
    "# Prepares the index path and avoid errors with already existing indices\n",
    "def prepare_index_path(indexName):\n",
    "    global index_count\n",
    "    index_count = index_count + 1\n",
    "    index_path = 'indices/' + indexName + str(index_count)\n",
    "\n",
    "    if os.path.exists(index_path) & os.path.isdir(index_path):\n",
    "        files = os.listdir(index_path)\n",
    "        for file in files:\n",
    "            file_name = index_path + '/' + file\n",
    "            os.remove(file_name)\n",
    "        os.rmdir(index_path)\n",
    "    elif os.path.exists(index_path) & (not os.path.isdir(index_path)):\n",
    "        os.rmove(index_path)\n",
    "\n",
    "    return os.path.abspath(index_path)\n",
    "\n",
    "def build_index(indexName, dataset):\n",
    "    index_path = prepare_index_path(indexName)\n",
    "    indexer = pt.IterDictIndexer(\n",
    "        index_path, overwrite=True, blocks=True)\n",
    "    indexer.setProperty(\n",
    "        \"stopwords.filename\", \"F:\\Bibliotheken\\Desktop\\Skripte\\packgaabwir2022\\en.txt\")\n",
    "    index_created = indexer.index(dataset.get_corpus_iter(),\n",
    "                            fields=['title', 'doi', 'abstract'],\n",
    "                            meta=('docno',))\n",
    "    return index_created"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vorgehen\n",
    "\n",
    "## Allgemein\n",
    "\n",
    "TREC-Covid-Queries verwenden und erweitern um weitere Anfrageterme, um die Retrieval-Performance zu verbessern (Query Expansion). \n",
    "\n",
    "Für die QE müssen Termkandidaten (\"set of C\" c_1, c_2, c_3, ...) bestimmt werden, die anschließend gerankt werden.\n",
    "D.h. ihr sendet zunächst die Standard-Query ab und später nochmal für die finale Evaluierung, die Query mit Termerweiterungen.\n",
    "\n",
    "## QE \n",
    "Wenn wenig Zeit:\n",
    "Nur die globale Variante evaluieren, wobei ja der \"Claim for Fame\" im Paper ist, dass die lokale Methode bessere Ergebnisse liefert.\n",
    "### LOKALE QE \n",
    "Word2Vec-Embeddings auf TREC-Covid trainieren (https://radimrehurek.com/gensim/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=tokens,window=10,sg=1,seed=1)\n",
    "model.save(\"word2vec_tokens.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 0, 'i': 1, 'e': 2, 'o': 3, 'r': 4, 'n': 5, 't': 6, 's': 7, 'c': 8, 'l': 9, 'm': 10, 'p': 11, 'u': 12, 'd': 13, 'h': 14, 'g': 15, 'b': 16, 'f': 17, 'v': 18, '1': 19, 'y': 20, '2': 21, 'k': 22, '0': 23, '3': 24, '4': 25, '5': 26, '6': 27, '9': 28, '7': 29, '8': 30, 'w': 31, 'z': 32, 'x': 33, 'j': 34, 'q': 35}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(str(model.wv.key_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Word2Vec' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mf:\\Bibliotheken\\Desktop\\Skripte\\packgaabwir2022\\approach.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Bibliotheken/Desktop/Skripte/packgaabwir2022/approach.ipynb#X53sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mimport\u001b[39;00m pyplot\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Bibliotheken/Desktop/Skripte/packgaabwir2022/approach.ipynb#X53sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# fit a 2d PCA model to the vectors\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/Bibliotheken/Desktop/Skripte/packgaabwir2022/approach.ipynb#X53sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m X \u001b[39m=\u001b[39m model[model\u001b[39m.\u001b[39;49mwv\u001b[39m.\u001b[39;49mkey_to_index]\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Bibliotheken/Desktop/Skripte/packgaabwir2022/approach.ipynb#X53sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m pca \u001b[39m=\u001b[39m PCA(n_components\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Bibliotheken/Desktop/Skripte/packgaabwir2022/approach.ipynb#X53sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m result \u001b[39m=\u001b[39m pca\u001b[39m.\u001b[39mfit_transform(X)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Word2Vec' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "# fit a 2d PCA model to the vectors\n",
    "X = model[model.wv.key_to_index]\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "# create a scatter plot of the projection\n",
    "pyplot.scatter(result[:, 0], result[:, 1])\n",
    "words = list(model.wv.key_to_index)\n",
    "for i, word in enumerate(words):\n",
    "    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "pyplot.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLOBALE QE\n",
    "Standardvariante wie z.B. auf Basis von Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Standard-Query an den Index für erstes Ranking\n",
    "Aus dem Title des Topics, quasi wie in der Standard-Pipeline in Pyterrier.\n",
    "Paper Inverse Document Frequency model (InL2)\n",
    "Zuerst BM25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create index only to retrieve the tokens from it\n",
    "dataset = pt.datasets.get_dataset('irds:cord19/trec-covid')\n",
    "trec_covid_index_ref = build_index('trecCovid',dataset)\n",
    "trec_covid_index = pt.IndexFactory.of(trec_covid_index_ref)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Erstellung der Termkandidaten\n",
    "## 2.1 Top-3 Dokumente\n",
    "Alle Terme (ausschließlich der Stoppworte)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Terme der Referenzen\n",
    "Alle Terme aus den Referenzen der 3 Dokumente (Terme der Titel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Erweiterung mit Co-Autoren\n",
    "Über die Koautoren können noch weitere Dokumente bzw. die entsprechenden Terme hinzugefügt werden. Aus Zeitgründen könnte man vielleicht auch diesen Schritt weglassen, wenn es zu viel Aufwand ist über Koautoren weitere Dokumente zu finden. Bei der Implementierung scheint ihr ja aber schon recht weit zu sein. Die \"relevanten Paper der Autoren\" sind einfach die Top-k Dokumente oder möglicherweise alle zusätzlichen Papers, die über die Koautorenschaft gefunden werden, gemeint. Daher ergibt sich auf der Name PSEUDO-Relevanz-Feedback, da einfach angenommen wird, dass die Top-Treffer alle relevant sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Ranking aller Terms\n",
    "Paper: Bo1\n",
    "Top-k Terme auswählen (k selbst wählen).\n",
    "Ergebnis: Potentielle Kandidaten für eine Termwerweiterung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5 Word2Vec-Modell mit Terms nachtrainieren\n",
    "Über die Kosinusähnlichkeit der Embeddings der ursprünglichen Anfrageterme bestimmt ihr nun weitere Terme aus dem Word2Vec-Modell. Diese Termkandidaten werden dann wie in 2.1.4 über Bo1 o.ä. gerankt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_model = Word2Vec.load(\"data/word2vec.model\")\n",
    "\n",
    "queries = preprocess_queries(queries, augmented=True, mv_model=mv_model)\n",
    "display(queries.head(5))\n",
    "\n",
    "pq_title = queries[['query_id', 'title']]\n",
    "pq_title = pq_title.rename(columns={'query_id':'qid', 'title':'query'})\n",
    "display(pq_title.head(5))\n",
    "\n",
    "pq_desc = queries[['query_id', 'description']]\n",
    "pq_desc = pq_desc.rename(columns={'query_id':'qid', 'description':'query'})\n",
    "display(pq_desc.head(5))\n",
    "\n",
    "pq_narr = queries[['query_id', 'narrative']]\n",
    "pq_narr = pq_narr.rename(columns={'query_id':'qid', 'narrative':'query'})\n",
    "display(pq_narr.head(5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.6 Zwei Sets zusammenfügen und Top-k für QE der ursprünglichen Querry nutzen\n",
    "Diese jeweils für die 50 Topics absenden.\n",
    "\n",
    "Ergebnis: Die finalen Rankings, die dann ausgewertet werden können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae50ba892a007039ee8b8cd0574fd987a1c048ef5b59b149546a5ae9fb6dc134"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
