{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Code (Run first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Capture the output but do not do anything with it\n",
    "# Just to avoid cluttering the output\n",
    "%pip install --upgrade git+https://github.com/terrierteam/pyterrier_doc2query.git\n",
    "%pip install python-terrier == 0.9.2\n",
    "%pip install semanticscholar\n",
    "%pip install gensim\n",
    "%pip install seaborn\n",
    "%pip install KrovetzStemmer\n",
    "%pip install nltk\n",
    "%pip install langdetect\n",
    "%pip install Unidecode\n",
    "\n",
    "\n",
    "# Load all packages and initialize pyTerrier\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyterrier as pt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from gensim.models import Word2Vec\n",
    "from semanticscholar import SemanticScholar\n",
    "from krovetzstemmer import Stemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from unidecode import unidecode\n",
    "\n",
    "from langdetect import detect\n",
    "krovetz_stemmer = Stemmer()\n",
    "regex_tokenizer = RegexpTokenizer(r'\\w+|\\$[\\d\\.]+|\\S+')\n",
    "\n",
    "if not pt.started():\n",
    "    pt.init()\n",
    "\n",
    "# Helper function to intialize multiple indices\n",
    "# Prepares the index path and avoid errors with already existing indices\n",
    "index_count = 0\n",
    "def prepare_index_path(indexName):\n",
    "    global index_count\n",
    "    index_count = index_count + 1\n",
    "    index_path = 'indices/' + indexName + str(index_count)\n",
    "\n",
    "    if os.path.exists(index_path) & os.path.isdir(index_path):\n",
    "        files = os.listdir(index_path)\n",
    "        for file in files:\n",
    "            file_name = index_path + '/' + file\n",
    "            os.remove(file_name)\n",
    "        os.rmdir(index_path)\n",
    "    elif os.path.exists(index_path) & (not os.path.isdir(index_path)):\n",
    "        os.rmove(index_path)\n",
    "\n",
    "    return os.path.abspath(index_path)\n",
    "\n",
    "def build_index(indexName, dataset):\n",
    "    index_path = prepare_index_path(indexName)\n",
    "    indexer = pt.IterDictIndexer(\n",
    "        index_path, overwrite=True, blocks=True)\n",
    "    indexer.setProperty(\n",
    "        \"stopwords.filename\", os.path.abspath(\"en.txt\"))\n",
    "    index_created = indexer.index(dataset.get_corpus_iter(),\n",
    "                            fields=['title', 'doi', 'abstract'],\n",
    "                            meta=('docno',))\n",
    "    return index_created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If not done already download the required data\n",
    "nltk.download()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vorgehen\n",
    "\n",
    "## Allgemein\n",
    "\n",
    "TREC-Covid-Queries verwenden und erweitern um weitere Anfrageterme, um die Retrieval-Performance zu verbessern (Query Expansion). \n",
    "\n",
    "Für die QE müssen Termkandidaten (\"set of C\" c_1, c_2, c_3, ...) bestimmt werden, die anschließend gerankt werden.\n",
    "D.h. ihr sendet zunächst die Standard-Query ab und später nochmal für die finale Evaluierung, die Query mit Termerweiterungen.\n",
    "\n",
    "## QE \n",
    "Wenn wenig Zeit:\n",
    "Nur die globale Variante evaluieren, wobei ja der \"Claim for Fame\" im Paper ist, dass die lokale Methode bessere Ergebnisse liefert.\n",
    "### LOKALE QE \n",
    "Word2Vec-Embeddings auf TREC-Covid trainieren (https://radimrehurek.com/gensim/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pt.datasets.get_dataset('irds:cord19/trec-covid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SMART Stopwordlist to remove stopwords\n",
    "with open(os.path.abspath(\"en.txt\")) as f:\n",
    "    stopword_list_lines = [line.rstrip('\\n') for line in f]\n",
    "stop_words = set(stopword_list_lines)\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in text.split() if word.lower() not in stop_words])\n",
    "\n",
    "# Preprocess the given text with SMART Stopword list, regex-Tokenizer and lowercase conversion\n",
    "def preprocess(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower()\n",
    "        sentence = remove_stopwords(sentence)\n",
    "\n",
    "    tokens = regex_tokenizer.tokenize(sentence)\n",
    "\n",
    "    result = []\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            result.append(krovetz_stemmer.stem(token))\n",
    "        except Exception as e:\n",
    "            result.append(krovetz_stemmer.stem(unidecode(a)))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = pd.DataFrame(dataset.get_corpus_iter())\n",
    "abstracts = docs[\"abstract\"]\n",
    "my_df = pd.DataFrame(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('en', 185008),\n",
       " ('de', 1820),\n",
       " ('fr', 1489),\n",
       " ('es', 1360),\n",
       " ('it', 522),\n",
       " ('nl', 389),\n",
       " ('ca', 338),\n",
       " ('pt', 285),\n",
       " ('ro', 224),\n",
       " ('af', 161),\n",
       " ('no', 108),\n",
       " ('tl', 96),\n",
       " ('da', 90),\n",
       " ('et', 72),\n",
       " ('zh-cn', 68),\n",
       " ('lt', 62),\n",
       " ('unknown', 61),\n",
       " ('id', 53),\n",
       " ('cy', 44),\n",
       " ('vi', 41)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# We can only use english because of english stopword list so we have to detect languages\n",
    "# And filter for english docs\n",
    "from collections import Counter\n",
    "from langdetect import detect\n",
    "\n",
    "languages = []\n",
    "for i, doc in docs.iterrows():\n",
    "    text = doc['abstract']\n",
    "    # If no abstract is present take the title for language detection#\n",
    "    # If the detection fails or there is not 'title' or 'abstract' set language to 'unkown'\n",
    "    if text is pd.NA or text == \"\":\n",
    "        text = doc['title']\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "    except Exception as e:\n",
    "        lang = 'unknown'\n",
    "    languages.append(lang)\n",
    "\n",
    "# convert the languages list to a numpy array\n",
    "languages = np.asarray(languages)\n",
    "\n",
    "docs_english = docs.loc[languages == \"en\"]\n",
    "docs_english.to_pickle(\"data/en_docs.pkl\")\n",
    "Counter(languages).most_common(20)\n",
    "# Load the english data\n",
    "# docs_english = pd.read_pickle(\"data/en_docs.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "abstracts = []\n",
    "\n",
    "docs_preprocessed = docs_englisch.copy()\n",
    "for index, row in tqdm(docs_preprocessed.iterrows(), total=len(docs_preprocessed)):\n",
    "    if row[\"title\"] is not pd.NA and not row[\"title\"] == \"\":\n",
    "      title_tokens = preprocess(row[\"title\"])\n",
    "      docs_preprocessed.loc[index, \"title\"] = \" \".join(title_tokens)\n",
    "      titles.extend(title_tokens)\n",
    "    if row[\"abstract\"] is not pd.NA and not row[\"abstract\"] == \"\":\n",
    "      abstract_tokens = preprocess(row[\"abstract\"])\n",
    "      docs_preprocessed.loc[index, \"abstract\"] = \" \".join(abstract_tokens)\n",
    "      abstracts.extend(abstract_tokens)\n",
    "docs_preprocessed.to_pickle(\"data/en_docs_preprocessed.pkl\")\n",
    "# Load the processed english data\n",
    "# en_docs_preprocessed = pd.read_pickle(\"data/en_docs_preprocessed.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_docs_preprocessed[\"sent_tokens\"] = en_docs_preprocessed.abstract\n",
    "en_docs_preprocessed.sent_tokens = en_docs_preprocessed.sent_tokens.apply(lambda input_text : [t.split() for t in  nltk.sent_tokenize(input_text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = en_docs_preprocessed.sent_tokens.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'str'>\n",
      "['conclusion', ':', 'result', 'similar', 'publish', 'data', 'finding', 'infection', 'common', 'infant', 'preschool', 'children', 'mortality', 'rate', 'pneumonia', 'patient', 'comorbid', 'high', '.']\n"
     ]
    }
   ],
   "source": [
    "print(type(data))\n",
    "print(type(data[0]))\n",
    "print(type(data[0][0]))\n",
    "print(data[0][:20])\n",
    "model = Word2Vec(sentences=data,window=10,sg=1,seed=1)\n",
    "model.save(\"data/word2vec_abstracts.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('-19', 0.7946233749389648)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('covid', topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Word2Vec' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mf:\\Bibliotheken\\Desktop\\Skripte\\packgaabwir2022\\approach.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Bibliotheken/Desktop/Skripte/packgaabwir2022/approach.ipynb#X53sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mimport\u001b[39;00m pyplot\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Bibliotheken/Desktop/Skripte/packgaabwir2022/approach.ipynb#X53sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# fit a 2d PCA model to the vectors\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/Bibliotheken/Desktop/Skripte/packgaabwir2022/approach.ipynb#X53sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m X \u001b[39m=\u001b[39m model[model\u001b[39m.\u001b[39;49mwv\u001b[39m.\u001b[39;49mkey_to_index]\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Bibliotheken/Desktop/Skripte/packgaabwir2022/approach.ipynb#X53sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m pca \u001b[39m=\u001b[39m PCA(n_components\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Bibliotheken/Desktop/Skripte/packgaabwir2022/approach.ipynb#X53sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m result \u001b[39m=\u001b[39m pca\u001b[39m.\u001b[39mfit_transform(X)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Word2Vec' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "# fit a 2d PCA model to the vectors\n",
    "X = model[model.wv.key_to_index]\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "# create a scatter plot of the projection\n",
    "pyplot.scatter(result[:, 0], result[:, 1])\n",
    "words = list(model.wv.key_to_index)\n",
    "for i, word in enumerate(words):\n",
    "    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "pyplot.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLOBALE QE\n",
    "Standardvariante wie z.B. auf Basis von Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Standard-Query an den Index für erstes Ranking\n",
    "Aus dem Title des Topics, quasi wie in der Standard-Pipeline in Pyterrier.\n",
    "Paper Inverse Document Frequency model (InL2)\n",
    "Zuerst BM25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create index only to retrieve the tokens from it\n",
    "dataset = pt.datasets.get_dataset('irds:cord19/trec-covid')\n",
    "trec_covid_index_ref = build_index('trecCovid',dataset)\n",
    "trec_covid_index = pt.IndexFactory.of(trec_covid_index_ref)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Erstellung der Termkandidaten\n",
    "## 2.1 Top-3 Dokumente\n",
    "Alle Terme (ausschließlich der Stoppworte)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Terme der Referenzen\n",
    "Alle Terme aus den Referenzen der 3 Dokumente (Terme der Titel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Erweiterung mit Co-Autoren\n",
    "Über die Koautoren können noch weitere Dokumente bzw. die entsprechenden Terme hinzugefügt werden. Aus Zeitgründen könnte man vielleicht auch diesen Schritt weglassen, wenn es zu viel Aufwand ist über Koautoren weitere Dokumente zu finden. Bei der Implementierung scheint ihr ja aber schon recht weit zu sein. Die \"relevanten Paper der Autoren\" sind einfach die Top-k Dokumente oder möglicherweise alle zusätzlichen Papers, die über die Koautorenschaft gefunden werden, gemeint. Daher ergibt sich auf der Name PSEUDO-Relevanz-Feedback, da einfach angenommen wird, dass die Top-Treffer alle relevant sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Ranking aller Terms\n",
    "Paper: Bo1\n",
    "Top-k Terme auswählen (k selbst wählen).\n",
    "Ergebnis: Potentielle Kandidaten für eine Termwerweiterung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5 Word2Vec-Modell mit Terms nachtrainieren\n",
    "Über die Kosinusähnlichkeit der Embeddings der ursprünglichen Anfrageterme bestimmt ihr nun weitere Terme aus dem Word2Vec-Modell. Diese Termkandidaten werden dann wie in 2.1.4 über Bo1 o.ä. gerankt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_model = Word2Vec.load(\"data/word2vec.model\")\n",
    "\n",
    "queries = preprocess_queries(queries, augmented=True, mv_model=mv_model)\n",
    "display(queries.head(5))\n",
    "\n",
    "pq_title = queries[['query_id', 'title']]\n",
    "pq_title = pq_title.rename(columns={'query_id':'qid', 'title':'query'})\n",
    "display(pq_title.head(5))\n",
    "\n",
    "pq_desc = queries[['query_id', 'description']]\n",
    "pq_desc = pq_desc.rename(columns={'query_id':'qid', 'description':'query'})\n",
    "display(pq_desc.head(5))\n",
    "\n",
    "pq_narr = queries[['query_id', 'narrative']]\n",
    "pq_narr = pq_narr.rename(columns={'query_id':'qid', 'narrative':'query'})\n",
    "display(pq_narr.head(5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.6 Zwei Sets zusammenfügen und Top-k für QE der ursprünglichen Querry nutzen\n",
    "Diese jeweils für die 50 Topics absenden.\n",
    "\n",
    "Ergebnis: Die finalen Rankings, die dann ausgewertet werden können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae50ba892a007039ee8b8cd0574fd987a1c048ef5b59b149546a5ae9fb6dc134"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
