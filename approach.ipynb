{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Code (Run first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Capture the output but do not do anything with it\n",
    "# Just to avoid cluttering the output\n",
    "%pip install --upgrade git+https: // github.com/terrierteam/pyterrier_doc2query.git\n",
    "%pip install python-terrier == 0.9.2\n",
    "%pip install semanticscholar\n",
    "%pip install gensim\n",
    "%pip install seaborn\n",
    "%pip install KrovetzStemmer\n",
    "%pip install nltk\n",
    "%pip install langdetect\n",
    "\n",
    "\n",
    "# Load all packages and initialize pyTerrier\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyterrier as pt\n",
    "import os\n",
    "from gensim.models import Word2Vec\n",
    "from semanticscholar import SemanticScholar\n",
    "from krovetzstemmer import Stemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from langdetect import detect\n",
    "\n",
    "krovetz_stemmer = Stemmer()\n",
    "regex_tokenizer = RegexpTokenizer(r'\\w+|\\$[\\d\\.]+|\\S+')\n",
    "\n",
    "if not pt.started():\n",
    "    pt.init()\n",
    "\n",
    "index_count = 0\n",
    "# Helper function to intialize multiple indices\n",
    "# Prepares the index path and avoid errors with already existing indices\n",
    "def prepare_index_path(indexName):\n",
    "    global index_count\n",
    "    index_count = index_count + 1\n",
    "    index_path = 'indices/' + indexName + str(index_count)\n",
    "\n",
    "    if os.path.exists(index_path) & os.path.isdir(index_path):\n",
    "        files = os.listdir(index_path)\n",
    "        for file in files:\n",
    "            file_name = index_path + '/' + file\n",
    "            os.remove(file_name)\n",
    "        os.rmdir(index_path)\n",
    "    elif os.path.exists(index_path) & (not os.path.isdir(index_path)):\n",
    "        os.rmove(index_path)\n",
    "\n",
    "    return os.path.abspath(index_path)\n",
    "\n",
    "def build_index(indexName, dataset):\n",
    "    index_path = prepare_index_path(indexName)\n",
    "    indexer = pt.IterDictIndexer(\n",
    "        index_path, overwrite=True, blocks=True)\n",
    "    indexer.setProperty(\n",
    "        \"stopwords.filename\", \"F:\\Bibliotheken\\Desktop\\Skripte\\packgaabwir2022\\en.txt\")\n",
    "    index_created = indexer.index(dataset.get_corpus_iter(),\n",
    "                            fields=['title', 'doi', 'abstract'],\n",
    "                            meta=('docno',))\n",
    "    return index_created"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vorgehen\n",
    "\n",
    "## Allgemein\n",
    "\n",
    "TREC-Covid-Queries verwenden und erweitern um weitere Anfrageterme, um die Retrieval-Performance zu verbessern (Query Expansion). \n",
    "\n",
    "Für die QE müssen Termkandidaten (\"set of C\" c_1, c_2, c_3, ...) bestimmt werden, die anschließend gerankt werden.\n",
    "D.h. ihr sendet zunächst die Standard-Query ab und später nochmal für die finale Evaluierung, die Query mit Termerweiterungen.\n",
    "\n",
    "## QE \n",
    "Wenn wenig Zeit:\n",
    "Nur die globale Variante evaluieren, wobei ja der \"Claim for Fame\" im Paper ist, dass die lokale Methode bessere Ergebnisse liefert.\n",
    "### LOKALE QE \n",
    "Word2Vec-Embeddings auf TREC-Covid trainieren (https://radimrehurek.com/gensim/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pt.datasets.get_dataset('irds:cord19/trec-covid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SMART Stopwordlist to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    with open('en.txt') as f:\n",
    "        lines = [line.rstrip('\\n') for line in f]\n",
    "    stop_words = set(lines)\n",
    "    return \" \".join([word for word in text.split() if word.lower() not in stop_words])\n",
    "# Preprocess the given text with SMART Stopword list, regex-Tokenizer and lowercase conversion\n",
    "def preprocess(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    tokens_list = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower()\n",
    "        sentence = remove_stopwords(sentence)\n",
    "\n",
    "    tokens = regex_tokenizer.tokenize(sentence)\n",
    "\n",
    "    result = []\n",
    "    for token in tokens:\n",
    "        result.append(krovetz_stemmer.stem(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cord19/trec-covid documents: 100%|██████████| 192509/192509 [00:01<00:00, 159974.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OBJECTIVE: This retrospective chart review describes the epidemiology and clinical features of 40 patients with culture-proven Mycoplasma pneumoniae infections at King Abdulaziz University Hospital, Jeddah, Saudi Arabia. METHODS: Patients with positive M. pneumoniae cultures from respiratory specimens from January 1997 through December 1998 were identified through the Microbiology records. Charts of patients were reviewed. RESULTS: 40 patients were identified, 33 (82.5%) of whom required admission. Most infections (92.5%) were community-acquired. The infection affected all age groups but was most common in infants (32.5%) and pre-school children (22.5%). It occurred year-round but was most common in the fall (35%) and spring (30%). More than three-quarters of patients (77.5%) had comorbidities. Twenty-four isolates (60%) were associated with pneumonia, 14 (35%) with upper respiratory tract infections, and 2 (5%) with bronchiolitis. Cough (82.5%), fever (75%), and malaise (58.8%) were the most common symptoms, and crepitations (60%), and wheezes (40%) were the most common signs. Most patients with pneumonia had crepitations (79.2%) but only 25% had bronchial breathing. Immunocompromised patients were more likely than non-immunocompromised patients to present with pneumonia (8/9 versus 16/31, P = 0.05). Of the 24 patients with pneumonia, 14 (58.3%) had uneventful recovery, 4 (16.7%) recovered following some complications, 3 (12.5%) died because of M pneumoniae infection, and 3 (12.5%) died due to underlying comorbidities. The 3 patients who died of M pneumoniae pneumonia had other comorbidities. CONCLUSION: our results were similar to published data except for the finding that infections were more common in infants and preschool children and that the mortality rate of pneumonia in patients with comorbidities was high.\n"
     ]
    }
   ],
   "source": [
    "docs = pd.DataFrame(dataset.get_corpus_iter())\n",
    "abstracts = docs[\"abstract\"]\n",
    "my_df = pd.DataFrame(abstracts)\n",
    "\n",
    "print(abstracts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = []\n",
    "for i, doc in docs.iterrows():\n",
    "    text = doc['abstract']\n",
    "\n",
    "    if text is pd.NA or text == \"\":\n",
    "        text = doc['title']\n",
    "    try:                                                          \n",
    "        lang = detect(text)                         \n",
    "    except Exception as e:                                                \n",
    "        lang='unknown'\n",
    "    languages.append(lang)\n",
    "\n",
    "languages = np.asarray(languages)\n",
    "v, c = np.unique(languages, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=abstracts,window=10,sg=1,seed=1)\n",
    "model.save(\"word2vec_abstracts.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('s', 0.9356473684310913)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('c', topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Word2Vec' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mf:\\Bibliotheken\\Desktop\\Skripte\\packgaabwir2022\\approach.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Bibliotheken/Desktop/Skripte/packgaabwir2022/approach.ipynb#X53sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mimport\u001b[39;00m pyplot\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Bibliotheken/Desktop/Skripte/packgaabwir2022/approach.ipynb#X53sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# fit a 2d PCA model to the vectors\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/Bibliotheken/Desktop/Skripte/packgaabwir2022/approach.ipynb#X53sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m X \u001b[39m=\u001b[39m model[model\u001b[39m.\u001b[39;49mwv\u001b[39m.\u001b[39;49mkey_to_index]\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Bibliotheken/Desktop/Skripte/packgaabwir2022/approach.ipynb#X53sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m pca \u001b[39m=\u001b[39m PCA(n_components\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Bibliotheken/Desktop/Skripte/packgaabwir2022/approach.ipynb#X53sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m result \u001b[39m=\u001b[39m pca\u001b[39m.\u001b[39mfit_transform(X)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Word2Vec' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "# fit a 2d PCA model to the vectors\n",
    "X = model[model.wv.key_to_index]\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "# create a scatter plot of the projection\n",
    "pyplot.scatter(result[:, 0], result[:, 1])\n",
    "words = list(model.wv.key_to_index)\n",
    "for i, word in enumerate(words):\n",
    "    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "pyplot.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLOBALE QE\n",
    "Standardvariante wie z.B. auf Basis von Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Standard-Query an den Index für erstes Ranking\n",
    "Aus dem Title des Topics, quasi wie in der Standard-Pipeline in Pyterrier.\n",
    "Paper Inverse Document Frequency model (InL2)\n",
    "Zuerst BM25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create index only to retrieve the tokens from it\n",
    "dataset = pt.datasets.get_dataset('irds:cord19/trec-covid')\n",
    "trec_covid_index_ref = build_index('trecCovid',dataset)\n",
    "trec_covid_index = pt.IndexFactory.of(trec_covid_index_ref)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Erstellung der Termkandidaten\n",
    "## 2.1 Top-3 Dokumente\n",
    "Alle Terme (ausschließlich der Stoppworte)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Terme der Referenzen\n",
    "Alle Terme aus den Referenzen der 3 Dokumente (Terme der Titel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Erweiterung mit Co-Autoren\n",
    "Über die Koautoren können noch weitere Dokumente bzw. die entsprechenden Terme hinzugefügt werden. Aus Zeitgründen könnte man vielleicht auch diesen Schritt weglassen, wenn es zu viel Aufwand ist über Koautoren weitere Dokumente zu finden. Bei der Implementierung scheint ihr ja aber schon recht weit zu sein. Die \"relevanten Paper der Autoren\" sind einfach die Top-k Dokumente oder möglicherweise alle zusätzlichen Papers, die über die Koautorenschaft gefunden werden, gemeint. Daher ergibt sich auf der Name PSEUDO-Relevanz-Feedback, da einfach angenommen wird, dass die Top-Treffer alle relevant sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Ranking aller Terms\n",
    "Paper: Bo1\n",
    "Top-k Terme auswählen (k selbst wählen).\n",
    "Ergebnis: Potentielle Kandidaten für eine Termwerweiterung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5 Word2Vec-Modell mit Terms nachtrainieren\n",
    "Über die Kosinusähnlichkeit der Embeddings der ursprünglichen Anfrageterme bestimmt ihr nun weitere Terme aus dem Word2Vec-Modell. Diese Termkandidaten werden dann wie in 2.1.4 über Bo1 o.ä. gerankt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_model = Word2Vec.load(\"data/word2vec.model\")\n",
    "\n",
    "queries = preprocess_queries(queries, augmented=True, mv_model=mv_model)\n",
    "display(queries.head(5))\n",
    "\n",
    "pq_title = queries[['query_id', 'title']]\n",
    "pq_title = pq_title.rename(columns={'query_id':'qid', 'title':'query'})\n",
    "display(pq_title.head(5))\n",
    "\n",
    "pq_desc = queries[['query_id', 'description']]\n",
    "pq_desc = pq_desc.rename(columns={'query_id':'qid', 'description':'query'})\n",
    "display(pq_desc.head(5))\n",
    "\n",
    "pq_narr = queries[['query_id', 'narrative']]\n",
    "pq_narr = pq_narr.rename(columns={'query_id':'qid', 'narrative':'query'})\n",
    "display(pq_narr.head(5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.6 Zwei Sets zusammenfügen und Top-k für QE der ursprünglichen Querry nutzen\n",
    "Diese jeweils für die 50 Topics absenden.\n",
    "\n",
    "Ergebnis: Die finalen Rankings, die dann ausgewertet werden können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae50ba892a007039ee8b8cd0574fd987a1c048ef5b59b149546a5ae9fb6dc134"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
